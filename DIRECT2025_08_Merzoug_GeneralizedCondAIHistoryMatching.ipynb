{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c438317",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "## DIRECT 6th Annual Consortium \n",
    "\n",
    "### Generalized Conditioning of Generative Artificial Intelligence for History Matching Subsurface Models\n",
    "\n",
    "#### Ahmed Merzoug, PhD Student, The University of Texas at Austin\n",
    "#### [LinkedIn](https://www.linkedin.com/in/ahmed-merzoug/) | [GitHub](https://github.com/amerzoug) | [GoogleScholar](https://scholar.google.com/citations?user=Ppx0Y1sAAAAJ&hl=en&oi=ao)\n",
    "\n",
    "#### Honggeun Jo, Assistant Professor, Inha University\n",
    "#### [LinkedIn](https://www.linkedin.com/in/honggeun-jo/) | [GoogleScholar](https://scholar.google.com/citations?user=u0OE5CIAAAAJ&hl=en)\n",
    "\n",
    "\n",
    "#### Michael Pyrcz, Professor,The University of Texas at Austin\n",
    "##### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1) | [GeostatsPy](https://github.com/GeostatsGuy/GeostatsPy)\n",
    "\n",
    "\n",
    "#### Work completed as part of the DIRECT consortium for Subsurface Data Analytics and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5144b5",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This Jupyter Notebook contains the Python code implementing the generalized conditioning approach for history matching subsurface models using Generative Artificial Intelligence (GenAI) and an Inference Network, as described in the associated research paper.\n",
    "\n",
    "The core idea is to train a primary GenAI model (specifically, a WGAN-GP) on **unconditioned** subsurface realizations. A separate, smaller Inference Network is then trained to map a latent space to the input space of the GenAI's generator such that the generated models are conditioned to specific well data (hard data and dynamic response). For history matching, the combined Inference Network and Generator are used within an Ensemble Smoother with Multiple Data Assimilation (ES-MDA) framework.\n",
    "\n",
    "The key advantage of this approach is its **generalizability**. When new well data becomes available (e.g., due to new wells being drilled), only the computationally inexpensive Inference Network needs to be retrained, while the expensive primary GenAI model remains fixed. This significantly reduces computational costs compared to methods that require retraining the entire generative model.\n",
    "\n",
    "The notebook demonstrates this workflow using a 3D fluvial channel reservoir case study. It simulates forward models using an external reservoir simulator (CMG) and assimilates dynamic production data (BHP) to update the latent space and generate history-matched reservoir models.\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "The code is designed to be run in a Jupyter Notebook environment. It requires several Python libraries and external components:\n",
    "\n",
    "1.  **Python Libraries:** `numpy`, `multiprocessing`, `time`, `os`, `torch`, `torch.nn`, `pandas`, `sklearn.metrics.mean_squared_error`, `pathlib`, `subprocess`, `scipy.interpolate`.\n",
    "2.  **External CMG Simulator:** The code interacts with the CMG IMEX simulator executable to run forward simulations. The path to the executable must be configured (`CMG` variable in `worker` function).\n",
    "3.  **`Sr3Reader.py` Module:** A custom Python module is required to read CMG simulation output files (`.sr3`). This module should contain `read_SR3` and `get_wells_timeseries` functions.\n",
    "4.  **Pre-trained Models:** The code loads a pre-trained Generator model (`checkpoint_epoch_*.pt`) and a pre-trained Inference Network model (`inference_net_epoch_*.pt`). These are assumed to be located in the `base_dir`.\n",
    "5.  **Simulation Template Files:** A base CMG input file template (`CMGBuilder00.dat`) and a file containing target days for interpolation (`Da2.csv`) are required in the `base_dir`.\n",
    "6.  **Truth Data:** CSV files containing the observed (truth) well production/pressure data (`Truth/Inj 1_CMG_aligned.csv`, `Truth/Prod 1_CMG_aligned.csv`, etc.) are required.\n",
    "\n",
    "### Code Functionality\n",
    "\n",
    "The notebook defines and executes the following steps:\n",
    "\n",
    "1.  **Setup and Configuration:** Imports necessary libraries, defines the Generator and Inference Network architectures (based on Appendix A and C of the paper), loads pre-trained models, and sets up key parameters in a `CONFIG` dictionary (paths, network dimensions, ESMDA settings).\n",
    "2.  **Helper Functions:**\n",
    "    *   `load_generator`: Loads a PyTorch Generator model from a checkpoint.\n",
    "    *   `flatteningRealization`: Reshapes a 3D realization into a 1D array slice by slice and maps facies values (0, 1, 2) to permeability values (0.01, 150, 50) suitable for the simulator input.\n",
    "    *   `create_dat_file`: Writes the flattened permeability data to a `.dat` file with a CMG-specific header.\n",
    "    *   `modify_dat_file`: Modifies the main CMG input template to include the generated permeability file name.\n",
    "    *   `worker`: This is the core simulation worker function. It takes a single realization, prepares the necessary CMG input files (`.dat`), runs the CMG simulator using `subprocess`, waits for the simulation output (`.sr3`), reads the output using `Sr3Reader`, extracts well timeseries data, interpolates it to target days (from `Da2.csv`), saves the original and aligned timeseries, and cleans up intermediate files. It includes error handling for simulation failure and file processing issues.\n",
    "    *   `threshold_samples`: Applies a 0.5 threshold to the continuous generator output to obtain discrete facies values (0 or 1).\n",
    "3.  **ESMDA Execution:**\n",
    "    *   The main execution block (`if __name__ == '__main__':`) orchestrates the ESMDA process.\n",
    "    *   **Load and Standardize Observations:** Reads the truth well data and standardizes it using its mean and standard deviation. This standardization is crucial for the assimilation step.\n",
    "    *   **Initialize Ensemble:** Initializes the latent variable ensemble (`z_ensemble`) from a standard normal distribution.\n",
    "    *   **ESMDA Loop:** Iterates for the specified number of ESMDA steps (`n_iter`).\n",
    "        *   **Map Latent to Generator Input:** Uses the `inference_net` to transform `z_ensemble` into `w_ensemble`. (`w_ensemble = inference_net(z_ensemble)`)\n",
    "        *   **Generate Models:** Uses the `generator` to create subsurface realizations from `w_ensemble`. (`generated_batch = generator(w_ensemble)`)\n",
    "        *   **Threshold Models:** Applies `threshold_samples` to get discrete facies/permeability models.\n",
    "        *   **Run Simulations:** Executes the `worker` function for each ensemble member in parallel using `multiprocessing`.\n",
    "        *   **Collect and Standardize Results:** Reads the aligned well timeseries data saved by the `worker` functions and standardizes it using the *truth* data's mean and standard deviation.\n",
    "        *   **Compute MSE:** Calculates the Mean Squared Error (MSE) between the *original* truth data and the *original* (unstandardized) simulation results for diagnostic purposes. Stores this history. Includes backing up and cleaning up simulation outputs.\n",
    "        *   **Compute Kalman Gain:** Calculates the sample covariances (`Cov_zd_std`, `Cov_dd_std`) between the latent variable anomalies (`A_z`) and the standardized simulation data anomalies (`A_d_std`), and computes the Kalman Gain (`K_std`).\n",
    "        *   **Update Latent Ensemble:** Applies the ESMDA update formula to each member of the latent ensemble (`z_ensemble`), incorporating a noisy observation (`d_obs_std + eps_std`) and the simulated data (`d_model_std[i]`).\n",
    "        *   **Save Ensembles:** Saves the updated `w_ensemble` and `z_ensemble` to `.pt` files, appending the iteration number.\n",
    "    *   **Save MSE History:** After the loop, saves the recorded MSE history to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f34385",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15975ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Sr3Reader module not found. Simulation output processing will fail.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import multiprocessing\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pathlib import Path\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import subprocess # Used for running external programs like the CMG simulator\n",
    "import shutil # Used for file operations\n",
    "# Libraries for data handling and interpolation\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "# Assuming Sr3Reader.py is available in the same directory or PYTHONPATH\n",
    "# This module is expected to provide functions for reading CMG .sr3 output files.\n",
    "# Error handling is included in the worker function for the import.\n",
    "try:\n",
    "    from Sr3Reader import read_SR3, get_wells_timeseries\n",
    "except ImportError:\n",
    "    print(\"Warning: Sr3Reader module not found. Simulation output processing will fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7dad17",
   "metadata": {},
   "source": [
    "#### Define Functions for History Matching "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3b4c4",
   "metadata": {},
   "source": [
    "\n",
    "This section defines components and utilities for a workflow involving geological model generation, processing, and reservoir simulation:\n",
    "\n",
    "1.  **`Generator` Class:** Defines a neural network that takes a low-dimensional latent vector and transforms it into a 3D geological volume using transposed 3D convolutions. It typically outputs continuous values (0-1 range) representing a geological property or facies probability.\n",
    "2.  **`load_generator` Function:** A utility to load a pre-trained instance of the `Generator` model from a saved file (`.pt` or `.pth`). It sets the model to evaluation mode.\n",
    "3.  **`InferenceNet` Class:** Defines a neural network that maps an input noise vector (`w`) to a latent vector (`z`). This network is designed to be trained to produce latent vectors suitable for the Generator, often conditioned on external data (as described in the previous context).\n",
    "4.  **Data Processing for Simulation:** Several functions handle converting the generated 3D volumes into a format required by reservoir simulators (like CMG):\n",
    "    *   **`threshold_samples`:** Converts continuous generator output into discrete values (e.g., facies indices like 0 or 1).\n",
    "    *   **`flatteningRealization`:** Takes a 3D array, flattens it slice-by-slice, and applies a mapping (e.g., facies to permeability values) to prepare it for simulation input.\n",
    "    *   **`create_dat_file`:** Writes the processed 1D data array into a `.dat` file with a specific header, suitable for simulator input.\n",
    "    *   **`modify_dat_file`:** Modifies a template simulation input file (`.dat`) to point to the newly created data file.\n",
    "5.  **Simulation Workflow:** The **`worker`** function encapsulates the process of taking a 3D realization, preparing its simulation input files, running an external reservoir simulator (using `subprocess`), processing the simulator's output (extracting and interpolating well timeseries data using external `Sr3Reader` functions and `interp1d`), and cleaning up intermediate files.\n",
    "\n",
    "In essence, this code provides the tools to generate 3D geological models, prepare them for reservoir simulation, run the simulation, and process the results, integrating a pre-trained generator and a potentially trained inference network into a simulation-based workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554b8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator network definition.\n",
    "    This network takes a latent vector (noise) and outputs a 3D volume realization.\n",
    "    It uses a series of ConvTranspose3d layers to upsample the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=100):\n",
    "        \"\"\"\n",
    "        Initializes the Generator network.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Dimension of the input latent vector.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Project and reshape the latent vector to an initial 3D feature map\n",
    "            nn.Linear(latent_dim, 512 * 2 * 8 * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # Unflatten the linear output into a 3D volume (channels, depth, height, width)\n",
    "            nn.Unflatten(1, (512, 2, 8, 8)),\n",
    "\n",
    "            # Upsampling block 1: Increase depth, height, and width by factor of 2\n",
    "            nn.ConvTranspose3d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(256), # Normalize across the batch and channels\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Upsampling block 2: Increase depth, height, and width by factor of 2\n",
    "            nn.ConvTranspose3d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Upsampling block 3: Increase depth, height, and width by factor of 2\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Final upsampling block: Output 1 channel.\n",
    "            # Kernel size (3, 4, 4), Stride (1, 2, 2). Stride 1 in depth direction, 2 in spatial.\n",
    "            nn.ConvTranspose3d(64, 1, kernel_size=(3, 4, 4), stride=(1, 2, 2), padding=1),\n",
    "            nn.Sigmoid() # Output values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the generator network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input latent vector (batch_size, latent_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Generated 3D volume (batch_size, 1, depth, height, width).\n",
    "                          Values are between 0 and 1.\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def load_generator(checkpoint_path: str, device: torch.device) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Load the trained generator from a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the generator checkpoint file (.pth).\n",
    "        device (torch.device): Device (e.g., 'cuda', 'cpu') to load the generator onto.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Loaded generator model in evaluation mode.\n",
    "    \"\"\"\n",
    "    # Initialize the generator model with default latent dimension\n",
    "    generator = Generator()\n",
    "    # Move the model to the specified device\n",
    "    generator.to(device)\n",
    "    \n",
    "    # Load the checkpoint file from disk\n",
    "    # map_location ensures the tensor is loaded onto the correct device,\n",
    "    # regardless of where it was saved.\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Check if the checkpoint is a dictionary containing the state dict\n",
    "    # (common practice when saving optimizer state, epoch, etc.)\n",
    "    if 'generator_state_dict' in checkpoint:\n",
    "        print(\"Loading generator state dict from checkpoint['generator_state_dict']\")\n",
    "        # Load the generator's state dictionary from the specific key\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    else:\n",
    "        print(\"Loading generator state dict directly from checkpoint (assuming it's just the state dict)\")\n",
    "        # Assume the checkpoint file *is* just the generator's state dict\n",
    "        generator.load_state_dict(checkpoint)\n",
    "    \n",
    "    # Set the model to evaluation mode (disables dropout, batch norm updates, etc.)\n",
    "    generator.eval()\n",
    "    print(f\"Generator loaded successfully from {checkpoint_path}\")\n",
    "    return generator\n",
    "\n",
    "def flatteningRealization(realization: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Flattens a 3D realization array slice by slice along the last axis (depth/z),\n",
    "    processing from the top slice (index shape[-1]-1) down to the bottom slice (index 0).\n",
    "    Applies a mapping to discrete values: 0 -> 0.01, 1 -> 150, 2 -> 50.\n",
    "    This format is often required by reservoir simulators like CMG.\n",
    "\n",
    "    Args:\n",
    "        realization (np.ndarray): A 3D NumPy array representing the realization\n",
    "                                 (e.g., shape [nx, ny, nz]).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 1D NumPy array containing the flattened and mapped values,\n",
    "                    ordered slice by slice from top to bottom.\n",
    "    \"\"\"\n",
    "    T = [] # List to store flattened slices\n",
    "    # Get the number of slices along the last dimension (depth)\n",
    "    num_slices = realization.shape[-1]\n",
    "    \n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] flatteningRealization: Input shape {realization.shape}. Processing {num_slices} slices.\", flush=True)\n",
    "\n",
    "    # Process slices in reverse order (from the last slice index down to 0)\n",
    "    # This assumes the last dimension corresponds to depth and simulation input\n",
    "    # expects data from top layers first. Check this assumption based on your simulator.\n",
    "    for reverse_idx in range(num_slices):\n",
    "        # Calculate the actual slice index (e.g., for num_slices=10, reverse_idx 0 -> idx 9, reverse_idx 9 -> idx 0)\n",
    "        idx = num_slices - 1 - reverse_idx\n",
    "        \n",
    "        # Extract the 2D slice at the current depth index\n",
    "        current_slice = realization[:, :, idx]\n",
    "\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] flatteningRealization: Processing slice {idx} (reverse order {reverse_idx})\", flush=True)\n",
    "\n",
    "        # Flatten the 2D slice into a 1D array (row by row)\n",
    "        flattened_array = current_slice.flatten()\n",
    "\n",
    "        # Apply the mapping based on facies values (assuming values 0, 1, 2 represent facies)\n",
    "        # Replace 0 with 0.01 (e.g., low permeability)\n",
    "        flattened_array = np.where(flattened_array == 0, 0.01, flattened_array)\n",
    "        # Replace 1 with 150 (e.g., high permeability)\n",
    "        flattened_array = np.where(flattened_array == 1, 150, flattened_array)\n",
    "        # Replace 2 with 50 (e.g., medium permeability)\n",
    "        flattened_array = np.where(flattened_array == 2, 50, flattened_array)\n",
    "        \n",
    "        # Append the processed and flattened slice to the list\n",
    "        T.append(flattened_array)\n",
    "    \n",
    "    # Concatenate all flattened slices into a single 1D array\n",
    "    result = np.concatenate(T)\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] flatteningRealization: Finished. Result length = {len(result)}\", flush=True)\n",
    "    return result\n",
    "\n",
    "def create_dat_file(filename: str, data_array: np.ndarray):\n",
    "    \"\"\"\n",
    "    Creates a .dat file formatted for reservoir simulation input (e.g., CMG).\n",
    "    Writes a header line followed by the data values, one value per line.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The full path and name of the .dat file to create.\n",
    "        data_array (np.ndarray): A 1D NumPy array containing the data values to write.\n",
    "    \"\"\"\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] create_dat_file: Creating file '{filename}' with {len(data_array)} values.\", flush=True)\n",
    "    try:\n",
    "        # Open the file in write mode ('w')\n",
    "        with open(filename, 'w') as f:\n",
    "            # Write the required header line for permeability data in CMG\n",
    "            f.write(\"*PERMI *ALL\\n\")\n",
    "            # Write each data value on a new line\n",
    "            for value in data_array:\n",
    "                f.write(str(value) + \"\\n\")\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] create_dat_file: '{filename}' created successfully.\", flush=True)\n",
    "    except Exception as e:\n",
    "        # Print an error message if file creation fails\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] create_dat_file: Error creating '{filename}': {e}\", flush=True)\n",
    "\n",
    "def modify_dat_file(input_file: str, output_file: str, old_keyword: str, new_keyword: str):\n",
    "    \"\"\"\n",
    "    Reads the content of an input file, replaces all occurrences of a specific\n",
    "    string (old_keyword) with another string (new_keyword), and writes the\n",
    "    modified content to an output file.\n",
    "\n",
    "    This is typically used to update a master simulation input file (.dat)\n",
    "    to point to a newly created data file (e.g., the permeability file).\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the original template .dat file.\n",
    "        output_file (str): Path where the modified .dat file will be saved.\n",
    "        old_keyword (str): The string to search for and replace (e.g., a placeholder filename).\n",
    "        new_keyword (str): The string to replace with (e.g., the actual filename).\n",
    "    \"\"\"\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] modify_dat_file: Modifying file '{input_file}' and saving to '{output_file}'.\", flush=True)\n",
    "    \n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] modify_dat_file: ERROR - The input file '{input_file}' does not exist.\", flush=True)\n",
    "        return # Exit the function if the input file is not found\n",
    "        \n",
    "    try:\n",
    "        # Open the input file in read mode ('r')\n",
    "        with open(input_file, 'r') as file:\n",
    "            # Read the entire content of the file\n",
    "            data = file.read()\n",
    "            \n",
    "        # Perform the string replacement\n",
    "        modified_data = data.replace(old_keyword, new_keyword)\n",
    "        \n",
    "        # Open the output file in write mode ('w')\n",
    "        with open(output_file, 'w') as file:\n",
    "            # Write the modified content to the output file\n",
    "            file.write(modified_data)\n",
    "            \n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] modify_dat_file: Successfully replaced '{old_keyword}' with '{new_keyword}'.\", flush=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Print an error message if reading, replacing, or writing fails\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] modify_dat_file: Error during modification: {e}\", flush=True)\n",
    "\n",
    "\n",
    "def worker(i: int, realization: np.ndarray):\n",
    "    \"\"\"\n",
    "    Worker function designed to process a single 3D realization, simulate\n",
    "    reservoir flow using an external simulator (CMG), extract well data\n",
    "    from the simulation output, process and save the data, and clean up\n",
    "    intermediate files. This function is intended to be potentially run\n",
    "    for multiple realizations, possibly in parallel.\n",
    "\n",
    "    Args:\n",
    "        i (int): An index or identifier for this specific realization/worker.\n",
    "                 Used for generating unique filenames.\n",
    "        realization (np.ndarray): A 3D NumPy array representing the realization.\n",
    "                                  Expected shape [nx, ny, nz].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Started processing realization.\", flush=True)\n",
    "        start_time = time.time() # Record start time for performance tracking\n",
    "        \n",
    "        # --- Configuration ---\n",
    "        # Path to the CMG IMEX executable\n",
    "        CMG = \"C:\\\\Program Files\\\\CMG\\\\IMEX\\\\2023.30\\\\Win_x64\\\\EXE\\\\mx202330.exe\"\n",
    "        \n",
    "\n",
    "        # Base directory for input/output files\n",
    "        base_dir = \"D:/Generalizedized_GAN/ESMDA/\"\n",
    "        \n",
    "        # Template CMG input file (needs to be modified)\n",
    "        template_file = os.path.join(base_dir, 'CMGBuilder00.dat')\n",
    "\n",
    "        # Directories for saving well timeseries data\n",
    "        output_dir = os.path.join(base_dir, 'wells_timeseries')\n",
    "        alined_dir = os.path.join(base_dir, 'wells_timeseries_alined')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(alined_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "        # --- Prepare Permeability Input File ---\n",
    "        # Flatten and map the realization data to permeability values for CMG input\n",
    "        data = flatteningRealization(realization)\n",
    "        \n",
    "        # Define the unique filename for the permeability input file\n",
    "        dat_filename = os.path.join(base_dir, f\"IN_PERM_X{i}.DAT\")\n",
    "        \n",
    "        # Create the .dat file with the processed permeability data\n",
    "        create_dat_file(dat_filename, data)\n",
    "        \n",
    "        # --- Modify Simulation Input File ---\n",
    "        # Define the output file path for the modified CMG input file\n",
    "        output_file = os.path.join(base_dir, f\"Ensemble{i}.dat\")\n",
    "        # Define the placeholder keyword in the template and the new keyword (the actual filename)\n",
    "        old_keyword = 'IN_PERM_X.DAT' # This keyword is expected in CMGBuilder00.dat\n",
    "        new_keyword = f'IN_PERM_X{i}.DAT' # This is the name of the file we just created\n",
    "        \n",
    "        # Modify the template file to point to the new permeability file and save as Ensemble{i}.dat\n",
    "        modify_dat_file(template_file, output_file, old_keyword, new_keyword)\n",
    " \n",
    "        # --- Run Simulation ---\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Running CMG simulation with '{output_file}'.\", flush=True)\n",
    "        # Execute the CMG simulator. The -f flag specifies the input file, -np 1 specifies 1 processor.\n",
    "        # subprocess.call waits for the command to complete.\n",
    "        # Assumes CMG executable is in the system's PATH or full path is provided.\n",
    "        # Assumes the command is correctly formed for your CMG installation.\n",
    "        ret_code = subprocess.call([CMG, \"-f\", output_file, \"-np\", \"1\"])\n",
    "        \n",
    "        # Check the return code of the subprocess call\n",
    "        if ret_code != 0:\n",
    "            # If the return code is non-zero, the simulation failed\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Simulation failed with return code {ret_code}. Skipping SR3 processing and cleanup.\", flush=True)\n",
    "            # Optionally, clean up input files even on failure:\n",
    "            # if os.path.exists(dat_filename): os.remove(dat_filename)\n",
    "            # if os.path.exists(output_file): os.remove(output_file)\n",
    "            return # Exit the worker function if simulation failed\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: CMG simulation completed successfully (return code {ret_code}).\", flush=True)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # --- Process Simulation Output (.sr3 file) ---\n",
    "        # -------------------------------------------------\n",
    "        # We assume that running the simulation with input file \"Ensemble{i}.dat\"\n",
    "        # automatically produces an SR3 file named \"Ensemble{i}.sr3\" in the same directory.\n",
    "        sr3_filename = os.path.join(base_dir, f\"Ensemble{i}.sr3\")\n",
    "        \n",
    "        # Wait loop to ensure the SR3 file has been created by the simulator\n",
    "        timeout = 300 # Max time to wait for SR3 file (e.g., 5 minutes)\n",
    "        wait_interval = 5 # seconds between checks\n",
    "        waited_time = 0\n",
    "        \n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Waiting for SR3 file '{sr3_filename}'.\", flush=True)\n",
    "        while not os.path.exists(sr3_filename) and waited_time < timeout:\n",
    "            time.sleep(wait_interval)\n",
    "            waited_time += wait_interval\n",
    "            print(f\"[{time.strftime('%H:%H:%S')}] Worker {i}: Waited {waited_time}/{timeout}s for '{sr3_filename}'.\", flush=True)\n",
    "            \n",
    "        # Check if the SR3 file exists after waiting\n",
    "        if os.path.exists(sr3_filename):\n",
    "            try:\n",
    "                # Check if the Sr3Reader functions were successfully imported earlier\n",
    "                if read_SR3 is None or get_wells_timeseries is None:\n",
    "                     raise ImportError(\"Sr3Reader functions not available.\")\n",
    "\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: SR3 file found. Processing '{sr3_filename}'.\", flush=True)\n",
    "                # Read the SR3 file content using the Sr3Reader library\n",
    "                sr3 = read_SR3(sr3_filename)\n",
    "                \n",
    "                # Extract well timeseries data from the SR3 object\n",
    "                wells_ts = get_wells_timeseries(sr3)\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Extracted timeseries data for {len(wells_ts)} wells.\", flush=True)\n",
    "                # print(wells_ts) # Optional: print extracted data structure\n",
    "\n",
    "                # --- Save and Process Well Timeseries Data ---\n",
    "                # Define a unique identifier for this simulation run\n",
    "                run_id = f\"Ensemble{i}\"\n",
    "                \n",
    "                # Load target days for interpolation (assuming Da2.csv exists and has a 'Days' column)\n",
    "                try:\n",
    "                    new_days_df = pd.read_csv(os.path.join(base_dir, \"Da2.csv\"))\n",
    "                    new_days = new_days_df[\"Days\"].values\n",
    "                    print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Loaded {len(new_days)} target days for interpolation.\", flush=True)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: ERROR - Da2.csv not found at {os.path.join(base_dir, 'Da2.csv')}. Skipping interpolation.\", flush=True)\n",
    "                    new_days = None # Set to None to skip interpolation\n",
    "\n",
    "                # Iterate through each well's timeseries data\n",
    "                for well_name, ts_data in wells_ts.items():\n",
    "                    print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Processing well '{well_name}'.\", flush=True)\n",
    "                    \n",
    "                    # Ensure the 'Days' column exists and is sorted\n",
    "                    if \"Days\" not in ts_data.columns:\n",
    "                         print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: WARNING - 'Days' column not found for well '{well_name}'. Skipping.\", flush=True)\n",
    "                         continue # Skip this well if 'Days' column is missing\n",
    "\n",
    "                    # Sort data by Days, just in case\n",
    "                    ts_data = ts_data.sort_values(by=\"Days\").reset_index(drop=True)\n",
    "\n",
    "                    # Save the original extracted timeseries data as a CSV\n",
    "                    unique_filename = f\"{well_name}_{run_id}.csv\"\n",
    "                    output_path = os.path.join(output_dir, unique_filename)\n",
    "                    ts_data.to_csv(output_path, index=False)\n",
    "                    print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Saved original timeseries for '{well_name}' to '{output_path}'.\", flush=True)\n",
    "\n",
    "                    # --- Interpolate Timeseries Data ---\n",
    "                    if new_days is not None:\n",
    "                        # Define the columns from the timeseries data that need to be interpolated\n",
    "                        columns_to_model = [\"BHP\", \"OILVOLSC\", \"GASVOLSC\", \"WATVOLSC\", \"LIQVOLSC\", \n",
    "                                          \"OILRATSC\", \"GASRATSC\", \"WATRATSC\", \"LIQRATSC\"]\n",
    "                        \n",
    "                        # Filter for columns actually present in the DataFrame\n",
    "                        columns_to_interpolate = [col for col in columns_to_model if col in ts_data.columns]\n",
    "                        \n",
    "                        if not columns_to_interpolate:\n",
    "                            print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: WARNING - None of the target columns {columns_to_model} found for well '{well_name}'. Skipping interpolation.\", flush=True)\n",
    "                        else:\n",
    "                            # Prepare dictionary to hold interpolated results\n",
    "                            predictions = {\"Days\": new_days}\n",
    "                            \n",
    "                            # Perform linear interpolation for each specified column\n",
    "                            source_days = ts_data[\"Days\"].values # Original days from simulation output\n",
    "                            \n",
    "                            for col in columns_to_interpolate:\n",
    "                                try:\n",
    "                                    # Create an interpolation function. fill_value='extrapolate' handles days outside the original range.\n",
    "                                    f = interp1d(source_days, ts_data[col].values, \n",
    "                                               kind='linear', fill_value='extrapolate')\n",
    "                                    # Apply the interpolation function to the new target days\n",
    "                                    predictions[col] = f(new_days)\n",
    "                                except ValueError as e:\n",
    "                                    print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: ERROR - Interpolation failed for well '{well_name}', column '{col}': {e}. Data might not be suitable for interpolation.\", flush=True)\n",
    "                                    # Skip interpolation for this column but continue with others\n",
    "                                    predictions[col] = np.nan # Or handle appropriately\n",
    "\n",
    "                            # Save the interpolated (aligned) predictions as a CSV\n",
    "                            predictions_df = pd.DataFrame(predictions)\n",
    "                            unique_filename2 = f\"{well_name}_{run_id}_Aligned.csv\"\n",
    "                            predictions_output_path = os.path.join(alined_dir, unique_filename2)\n",
    "                            predictions_df.to_csv(predictions_output_path, index=False)\n",
    "                            print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Saved aligned timeseries for '{well_name}' to '{predictions_output_path}'.\", flush=True)\n",
    "\n",
    "\n",
    "                # --- Clean Up Intermediate Files ---\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Cleaning up intermediate files.\", flush=True)\n",
    "                del sr3 # Release the SR3 object memory\n",
    "                # Remove the SR3 output file\n",
    "                if os.path.exists(sr3_filename):\n",
    "                     os.remove(sr3_filename)\n",
    "                     print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Removed '{sr3_filename}'.\", flush=True)\n",
    "                # Remove the generated permeability input file\n",
    "                if os.path.exists(dat_filename):\n",
    "                     os.remove(dat_filename)\n",
    "                     print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Removed '{dat_filename}'.\", flush=True)\n",
    "                # Remove the modified simulation input file\n",
    "                if os.path.exists(output_file):\n",
    "                     os.remove(output_file)\n",
    "                     print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Removed '{output_file}'.\", flush=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch any errors during SR3 processing or file handling\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Exception occurred during SR3 processing or cleanup: {e}\", flush=True)\n",
    "                # Note: Files might not be cleaned up if an error occurs here.\n",
    "\n",
    "        else:\n",
    "            # Message if the SR3 file was not found after the timeout\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: ERROR - SR3 file '{sr3_filename}' not found after simulation or timeout.\", flush=True)\n",
    "            # Optionally, clean up input files even if SR3 wasn't found:\n",
    "            # if os.path.exists(dat_filename): os.remove(dat_filename)\n",
    "            # if os.path.exists(output_file): os.remove(output_file)\n",
    "        \n",
    "        # Calculate and print total time taken for this worker\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: Finished processing in {elapsed:.2f} seconds.\", flush=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any errors that occurred outside the specific SR3 processing block\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Worker {i}: An unexpected Exception occurred: {e}\", flush=True)\n",
    "\n",
    "\n",
    "def threshold_samples(samples: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies a simple binary threshold to the generated samples.\n",
    "    Values below 0.5 are set to 0, and values >= 0.5 are set to 1.\n",
    "    This is typically used to convert continuous generator outputs (0-1 range)\n",
    "    into discrete facies indices (e.g., 0 or 1).\n",
    "\n",
    "    Args:\n",
    "        samples (np.ndarray): A NumPy array of continuous values (e.g., output of a Sigmoid).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array with values thresholded to 0 or 1.\n",
    "    \"\"\"\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] threshold_samples: Applying binary threshold at 0.5.\", flush=True)\n",
    "    # Use numpy's where function for efficient element-wise conditional replacement\n",
    "    # The condition is `samples >= 0.5`.\n",
    "    # If True, assign 1.\n",
    "    # If False, assign 0.\n",
    "    samples = np.where(samples >= 0.5, 1, 0)\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] threshold_samples: Thresholding complete. Values are now 0 or 1.\", flush=True)\n",
    "    return samples\n",
    "\n",
    "class InferenceNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Inference network that transforms an input noise vector (w) into an output\n",
    "    latent vector (z). The intention is typically for 'z' to have properties\n",
    "    (e.g., Gaussian) that make it suitable as input to a generator, potentially\n",
    "    conditioned on external data (though the conditioning data input is missing\n",
    "    in this network definition).\n",
    "    \n",
    "    The architecture uses a sequence of linear layers with BatchNorm and SELU activations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w_dim: int = 100, z_dim: int = 100, hidden_dim: int = 256):\n",
    "        \"\"\"\n",
    "        Initializes the Inference network.\n",
    "        \n",
    "        Args:\n",
    "            w_dim (int): Dimension of the input noise vector (w).\n",
    "            z_dim (int): Dimension of the output latent vector (z).\n",
    "            hidden_dim (int): Dimension of the hidden layers.\n",
    "        \"\"\"\n",
    "        super(InferenceNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Linear layer to project input noise to hidden dimension\n",
    "            nn.Linear(w_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), # Batch normalization\n",
    "            nn.SELU(), # Scaled Exponential Linear Unit activation\n",
    "\n",
    "            # Additional hidden layers\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SELU(),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SELU(),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SELU(),\n",
    "\n",
    "            # Final linear layer to project to the output latent dimension\n",
    "            nn.Linear(hidden_dim, z_dim)  # Output is a vector of size z_dim\n",
    "        )\n",
    "        \n",
    "        # Initialize network weights for potentially better convergence\n",
    "        self._init_weights()\n",
    "        print(f\"InferenceNet initialized with w_dim={w_dim}, z_dim={z_dim}, hidden_dim={hidden_dim}\")\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initializes network weights using Xavier Normal and biases to zeros.\"\"\"\n",
    "        print(\"Initializing InferenceNet weights...\")\n",
    "        for m in self.modules(): # Iterate through all modules in the network\n",
    "            if isinstance(m, nn.Linear): # Check if the module is a Linear layer\n",
    "                # Initialize weights using Xavier Normal distribution\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                # Initialize biases to zeros\n",
    "                nn.init.zeros_(m.bias)\n",
    "        print(\"InferenceNet weights initialized.\")\n",
    "        \n",
    "    def forward(self, w: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the Inference network.\n",
    "        \n",
    "        Args:\n",
    "            w (torch.Tensor): Input noise vector (batch_size, w_dim), typically from a Gaussian distribution.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output latent vector (batch_size, z_dim).\n",
    "        \"\"\"\n",
    "        # Pass the input noise vector through the sequential network\n",
    "        z = self.net(w)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261bd10",
   "metadata": {},
   "source": [
    "#### Training the Inference Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f22cb",
   "metadata": {},
   "source": [
    "\n",
    "This section focuses on generating diverse geological realizations that honor observed data at specific well locations. It achieves this by training a separate **Inference Network** while using a pre-trained, **frozen** image **Generator**.\n",
    "\n",
    "1.  **Frozen Generator:** A pre-trained generator model is loaded and its parameters are fixed. It acts as a fixed mapping from latent vectors (`z`) to geological images.\n",
    "2.  **Inference Network:** A new neural network (`InferenceNet`) is defined and trained. It takes simple random noise (`w`) as input and outputs a latent vector (`z`).\n",
    "3.  **Conditioning Data:** Observed geological values (`d_obs`) at specific well locations are loaded and used as the conditioning data.\n",
    "4.  **Training Objective (Cost Function):** The `InferenceNet` is trained to make the distribution of generated `z` vectors approximate the desired posterior distribution `P(z | d_obs)`. The loss function minimizes the difference between the approximate posterior `Q(z|w)` and the true posterior `P(z|d_obs)`. This involves three key components:\n",
    "    *   **Well Likelihood:** Measures how well the images produced by the *frozen* generator (using `z` from the Inference Net) match the observed data at the well locations. This term drives the network to generate data-consistent images.\n",
    "    *   **Gaussian Prior:** Encourages the latent vectors `z` to follow a standard Gaussian distribution, regularizing the latent space.\n",
    "    *   **Entropy Estimation:** Estimates the entropy of the generated `z` distribution using a k-nearest neighbor method. This term promotes diversity in the generated `z` vectors, ensuring a wide range of plausible realizations are sampled.\n",
    "5.  **Outcome:** After training, the `InferenceNet` can be used to sample new random noise vectors (`w`) and transform them into conditioned latent vectors (`z`). Feeding these `z` vectors into the frozen generator produces diverse geological realizations that are consistent with the observed well data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba718fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    \"base_dir\": \"D:/DA Project3/Corrected Bew results\",\n",
    "    \"checkpoint_path\": \"checkpoint_epoch_3000.pt\",\n",
    "    \n",
    "    \n",
    "    # Well locations (row, col) format\n",
    "    \"well_locations\": [(59, 37), (86, 40), (109, 39), (72, 72), (20,80), (95, 95) ],\n",
    "    \n",
    "    # Network parameters\n",
    "    \"w_dim\": 100,          # Noise dimension\n",
    "    \"z_dim\": 100,          # Latent dimension\n",
    "    \"hidden_dim\": 256,     # Hidden layer dimension\n",
    "    \n",
    "    # Training parameters\n",
    "    \"alpha\": 10,            # Likelihood variance\n",
    "    \"lr\": 1e-4,            # Learning rate\n",
    "    \"batch_size\": 50,      # Batch size\n",
    "    \"total_cases\": 1000,   # Total number of generated examples\n",
    "    \"num_epochs\": 400,     # Number of epochs\n",
    "    \"log_interval\": 10,    # Log every N batches\n",
    "    \n",
    "    # Threshold parameters\n",
    "    \"threshold\": 0.2,      # Threshold value for binary classification\n",
    "}\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "###################################\n",
    "# 1. Load the Frozen Generator\n",
    "###################################\n",
    "checkpoint_path = os.path.join(CONFIG[\"base_dir\"], CONFIG[\"checkpoint_path\"])\n",
    "generator = load_generator(checkpoint_path, device)\n",
    "generator.eval()  # Set to evaluation mode\n",
    "\n",
    "# Freeze generator parameters\n",
    "for param in generator.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Generator loaded and parameters frozen.\")\n",
    "\n",
    "###################################\n",
    "# 2. Load Observations (Well Data)\n",
    "###################################\n",
    "data_path = os.path.join(CONFIG[\"base_dir\"], CONFIG[\"data_path\"])\n",
    "CA = np.load(data_path)\n",
    "Truth = CA.transpose(1, 2, 0)  # shape: [H, W, Channels]\n",
    "\n",
    "# Extract observations at well locations\n",
    "facies_obs = []\n",
    "for row, col in CONFIG[\"well_locations\"]:\n",
    "    facies_obs.append(Truth[row, col, :])\n",
    "\n",
    "facies_obs = np.stack(facies_obs, axis=0)\n",
    "d_obs = torch.tensor(facies_obs, dtype=torch.float32, device=device)\n",
    "\n",
    "print(f\"Shape of facies_obs (num_wells, Channels) = {facies_obs.shape}\")\n",
    "print(f\"Shape of d_obs = {d_obs.shape}\")\n",
    "\n",
    "###################################\n",
    "# 3. Define the Inference Network for Gaussian Sampling\n",
    "###################################\n",
    "class InferenceNet(nn.Module):\n",
    "    def __init__(self, w_dim: int = 100, z_dim: int = 100, hidden_dim: int = 256):\n",
    "        \"\"\"\n",
    "        Inference network that transforms noise vectors while maintaining Gaussian properties.\n",
    "        The network takes random Gaussian noise as input and transforms it into a new Gaussian\n",
    "        that is conditioned on the well data.\n",
    "        \n",
    "        Args:\n",
    "            w_dim: Dimension of input noise vector\n",
    "            z_dim: Dimension of output latent vector\n",
    "            hidden_dim: Dimension of hidden layers\n",
    "        \"\"\"\n",
    "        super(InferenceNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(w_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, z_dim)  # Output is a vector of size z_dim\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize network weights for better convergence.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self, w: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Transform input noise to output latent vector.\n",
    "        \n",
    "        Args:\n",
    "            w: Input noise vector with Gaussian distribution\n",
    "            \n",
    "        Returns:\n",
    "            z: Output latent vector that maintains Gaussian properties\n",
    "        \"\"\"\n",
    "        z = self.net(w)\n",
    "        return z\n",
    "\n",
    "# Initialize inference network\n",
    "inference_net = InferenceNet(\n",
    "    CONFIG[\"w_dim\"], \n",
    "    CONFIG[\"z_dim\"], \n",
    "    CONFIG[\"hidden_dim\"]\n",
    ").to(device)\n",
    "\n",
    "print(f\"InferenceNet initialized with output size {CONFIG['z_dim']}\")\n",
    "\n",
    "###################################\n",
    "# 4. Define Loss Functions and Entropy Estimator\n",
    "###################################\n",
    "def well_likelihood(z_batch: torch.Tensor, alpha: float = CONFIG[\"alpha\"]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the well data likelihood for a batch of latent vectors z.\n",
    "    \n",
    "    Args:\n",
    "        z_batch: Batch of latent vectors, shape [batch_size, z_dim]\n",
    "        alpha: Likelihood variance\n",
    "        \n",
    "    Returns:\n",
    "        Log likelihood for each sample in the batch\n",
    "    \"\"\"\n",
    "    # Pass z through the generator\n",
    "    x = generator(z_batch).squeeze()  # Expected shape: [batch, Channels, H, W]\n",
    "    \n",
    "    # Extract simulated observations at the well locations\n",
    "    gen = x.permute(0, 2, 3, 1)  # [batch, H, W, Channels]\n",
    "    \n",
    "    # Collect simulated measurements at well locations\n",
    "    sim_obs_list = []\n",
    "    for row, col in CONFIG[\"well_locations\"]:\n",
    "        sim_obs_list.append(gen[:, row, col, :])\n",
    "    \n",
    "    # Stack to get shape [batch, num_wells, Channels]\n",
    "    sim_obs = torch.stack(sim_obs_list, dim=1)\n",
    "    \n",
    "    # Binary cross-entropy loss\n",
    "    bce_loss = F.binary_cross_entropy(sim_obs, d_obs.unsqueeze(0).expand_as(sim_obs), reduction='none')\n",
    "    log_likelihood = -bce_loss.view(z_batch.shape[0], -1).sum(dim=1) / alpha\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "def gaussian_prior(z_batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the standard Gaussian prior for a batch of latent vectors.\n",
    "    \n",
    "    Args:\n",
    "        z_batch: Batch of latent vectors\n",
    "        \n",
    "    Returns:\n",
    "        Log prior for each sample in the batch\n",
    "    \"\"\"\n",
    "    # Standard normal prior: -0.5 * sum(z^2)\n",
    "    log_prior = -0.5 * (z_batch**2).view(z_batch.shape[0], -1).sum(dim=1)\n",
    "    return log_prior\n",
    "\n",
    "def compute_statistics(z_batch: torch.Tensor) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute the mean, variance, and covariance of the batch.\n",
    "    \n",
    "    Args:\n",
    "        z_batch: Batch of latent vectors\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (mean, variance, covariance matrix)\n",
    "    \"\"\"\n",
    "    mean = torch.mean(z_batch, dim=0)\n",
    "    centered = z_batch - mean.unsqueeze(0)\n",
    "    variance = torch.mean(centered**2, dim=0)\n",
    "    cov = torch.matmul(centered.T, centered) / z_batch.shape[0]\n",
    "    return mean, variance, cov\n",
    "\n",
    "def pairwise_distances(z_batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute pairwise squared distances between all points in the batch.\n",
    "    \n",
    "    Args:\n",
    "        z_batch: Batch of vectors, shape [batch_size, dim]\n",
    "        \n",
    "    Returns:\n",
    "        Matrix of squared distances, shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Compute squared Euclidean distance between all pairs\n",
    "    # ||a - b||^2 = ||a||^2 + ||b||^2 - 2 * a  b\n",
    "    \n",
    "    # Compute squared norms for each vector: ||a||^2\n",
    "    z_norm = (z_batch**2).sum(1).view(-1, 1)\n",
    "    \n",
    "    # Transpose for matrix multiplication\n",
    "    z_t = torch.transpose(z_batch, 0, 1)\n",
    "    \n",
    "    # Transpose squared norms to match matrix shape: ||b||^2\n",
    "    z_t_norm = z_norm.view(1, -1)\n",
    "    \n",
    "    # Calculate ||a||^2 + ||b||^2 - 2 * a  b\n",
    "    dist = z_norm + z_t_norm - 2.0 * torch.mm(z_batch, z_t)\n",
    "    \n",
    "    # Ensure no negative distances due to numerical issues\n",
    "    dist = torch.clamp(dist, 0.0, float('inf'))\n",
    "    \n",
    "    return dist\n",
    "\n",
    "def kozachenko_leonenko_entropy(z_batch: torch.Tensor, k: int = 5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Estimate the entropy of a distribution using the Kozachenko-Leonenko estimator.\n",
    "    \n",
    "    Args:\n",
    "        z_batch: Batch of vectors sampled from distribution, shape [batch_size, dim]\n",
    "        k: Number of nearest neighbor to use (recommended: sqrt(batch_size))\n",
    "        \n",
    "    Returns:\n",
    "        Entropy estimate\n",
    "    \"\"\"\n",
    "    batch_size, dim = z_batch.shape\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    distances = pairwise_distances(z_batch)\n",
    "    \n",
    "    # Set diagonal to infinity (exclude self)\n",
    "    distances = distances + torch.diag(torch.ones(batch_size, device=z_batch.device) * float('inf'))\n",
    "    \n",
    "    # Get k-th smallest distance for each row\n",
    "    knn_dist, _ = torch.topk(distances, k=k, dim=1, largest=False)\n",
    "    \n",
    "    # Get the k-th nearest distance\n",
    "    kth_distance = knn_dist[:, k-1]\n",
    "    \n",
    "    # Log distance (with small epsilon for numerical stability)\n",
    "    log_dist = torch.log(kth_distance + 1e-8)\n",
    "    \n",
    "    # Entropy estimate (excluding constants)\n",
    "    # H = dim * mean(log(distance)) + constant\n",
    "    entropy = dim * torch.mean(log_dist)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "###################################\n",
    "# 5. Training Loop (Modified to include Entropy)\n",
    "###################################\n",
    "def train():\n",
    "    \"\"\"Execute the training loop for the inference network.\"\"\"\n",
    "    # Setup optimizer with improved settings\n",
    "    optimizer = optim.Adam(\n",
    "        inference_net.parameters(), \n",
    "        lr=CONFIG[\"lr\"], \n",
    "        amsgrad=True, \n",
    "        betas=(0.5, 0.9)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler for better convergence\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, 'min', patience=10, factor=0.5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Calculate batches per epoch\n",
    "    num_batches_per_epoch = CONFIG[\"total_cases\"] // CONFIG[\"batch_size\"]\n",
    "    batch_size = CONFIG[\"batch_size\"]\n",
    "    \n",
    "    # Track metrics\n",
    "    metrics = {\n",
    "        'epoch': [],\n",
    "        'loss': [],\n",
    "        'well_likelihood': [],\n",
    "        'prior': [],\n",
    "        'entropy': [],\n",
    "        'gaussian_metrics': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Starting training for {CONFIG['num_epochs']} epochs\")\n",
    "    print(f\"Batches per epoch: {num_batches_per_epoch}\")\n",
    "    print(f\"Using Kozachenko-Leonenko entropy estimator with k=sqrt(batch_size)\")\n",
    "    \n",
    "    # Get k for nearest neighbor entropy estimator (sqrt of batch size)\n",
    "    k = max(1, int(math.sqrt(batch_size)))\n",
    "    print(f\"Using k={k} for entropy estimation\")\n",
    "    \n",
    "    for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "        epoch_loss = 0\n",
    "        epoch_well_likelihood = 0\n",
    "        epoch_prior = 0\n",
    "        epoch_entropy = 0\n",
    "        epoch_gaussian_metrics = []\n",
    "        \n",
    "        for batch_idx in range(num_batches_per_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Sample random noise vector w ~ N(0,I)\n",
    "            w = torch.randn(batch_size, CONFIG[\"w_dim\"], device=device)\n",
    "            \n",
    "            # Get latent code from inference network\n",
    "            z = inference_net(w)  # [batch_size, z_dim]\n",
    "            \n",
    "            # Compute well data likelihood (negative log likelihood)\n",
    "            ll = well_likelihood(z)\n",
    "            \n",
    "            # Compute Gaussian prior\n",
    "            prior = gaussian_prior(z)\n",
    "            \n",
    "            # Compute expected negative log posterior (loss term)\n",
    "            expected_loss = -(ll + prior).mean()\n",
    "            \n",
    "            # Compute entropy using Kozachenko-Leonenko estimator\n",
    "            entropy = kozachenko_leonenko_entropy(z, k)\n",
    "            \n",
    "            # Total loss: expected loss (negative log posterior) - entropy\n",
    "            # This follows the Kullback-Leibler divergence minimization approach\n",
    "            # KL(q||p) = E_q[log q] - E_q[log p] = -H(q) + E_q[-log p]\n",
    "            total_loss = expected_loss - 0.5*entropy\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            total_loss.backward()\n",
    "            # After loss.backward() but before optimizer.step()\n",
    "            torch.nn.utils.clip_grad_norm_(inference_net.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute Gaussian metrics for monitoring\n",
    "            with torch.no_grad():\n",
    "                mean, variance, _ = compute_statistics(z)\n",
    "                mean_error = torch.mean(torch.abs(mean))\n",
    "                var_error = torch.mean(torch.abs(variance - 1.0))\n",
    "                epoch_gaussian_metrics.append((mean_error.item(), var_error.item()))\n",
    "            \n",
    "            # Update running metrics\n",
    "            epoch_loss += total_loss.item()\n",
    "            epoch_well_likelihood += ll.mean().item()\n",
    "            epoch_prior += prior.mean().item()\n",
    "            epoch_entropy += entropy.item()\n",
    "            \n",
    "            # Log progress\n",
    "            if batch_idx % CONFIG[\"log_interval\"] == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {batch_idx} | \"\n",
    "                      f\"well_ll: {ll.mean().item():.4f}, \"\n",
    "                      f\"prior: {prior.mean().item():.4f}, \"\n",
    "                      f\"entropy: {entropy.item():.4f}, \"\n",
    "                      f\"loss: {total_loss.item():.4f}\")\n",
    "        \n",
    "        # Calculate epoch averages\n",
    "        avg_loss = epoch_loss / num_batches_per_epoch\n",
    "        avg_well_likelihood = epoch_well_likelihood / num_batches_per_epoch\n",
    "        avg_prior = epoch_prior / num_batches_per_epoch\n",
    "        avg_entropy = epoch_entropy / num_batches_per_epoch\n",
    "        avg_mean_error, avg_var_error = np.mean(epoch_gaussian_metrics, axis=0)\n",
    "        \n",
    "        # Update metrics history\n",
    "        metrics['epoch'].append(epoch)\n",
    "        metrics['loss'].append(avg_loss)\n",
    "        metrics['well_likelihood'].append(avg_well_likelihood)\n",
    "        metrics['prior'].append(avg_prior)\n",
    "        metrics['entropy'].append(avg_entropy)\n",
    "        metrics['gaussian_metrics'].append((avg_mean_error, avg_var_error))\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Log epoch summary\n",
    "        print(f\"Epoch {epoch} Summary | \"\n",
    "              f\"Avg well_ll: {avg_well_likelihood:.4f}, \"\n",
    "              f\"Avg prior: {avg_prior:.4f}, \"\n",
    "              f\"Avg entropy: {avg_entropy:.4f}, \"\n",
    "              f\"Mean error: {avg_mean_error:.4f}, \"\n",
    "              f\"Var error: {avg_var_error:.4f}, \"\n",
    "              f\"Avg loss: {avg_loss:.4f}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save checkpoint every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            save_path = os.path.join(CONFIG[\"base_dir\"], f\"inference_net_epoch_{epoch+1}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': inference_net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics\n",
    "            }, save_path)\n",
    "            print(f\"Checkpoint saved at {save_path}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "###################################\n",
    "# 6. Visualization Functions\n",
    "###################################\n",
    "def generate_and_visualize(num_samples: int = 5):\n",
    "    \"\"\"Generate samples using the trained inference network and visualize results.\"\"\"\n",
    "    inference_net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate latent codes through the inference network\n",
    "        w = torch.randn(num_samples, CONFIG[\"w_dim\"], device=device)\n",
    "        z = inference_net(w)\n",
    "        \n",
    "        # Generate images\n",
    "        samples = generator(z).cpu().squeeze()\n",
    "        \n",
    "        # Plot generated samples\n",
    "        fig, axes = plt.subplots(1, num_samples, figsize=(num_samples*4, 4))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Assuming the output is in the format [channels, height, width]\n",
    "            # and you want to visualize the first channel\n",
    "            axes[i].imshow(samples[i, 15].cpu().numpy(), cmap='viridis')\n",
    "            axes[i].set_title(f\"Sample {i+1}\")\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CONFIG[\"base_dir\"], \"generated_samples.png\"))\n",
    "        plt.show()\n",
    "\n",
    "def visualize_latent_distribution(num_samples: int = 1000):\n",
    "    \"\"\"Visualize the distribution of the latent space to verify it's Gaussian.\"\"\"\n",
    "    inference_net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate latent codes through the inference network\n",
    "        w = torch.randn(num_samples, CONFIG[\"w_dim\"], device=device)\n",
    "        z = inference_net(w)\n",
    "        \n",
    "        # Move to CPU for visualization\n",
    "        z_np = z.cpu().numpy()\n",
    "        \n",
    "        # Check if the distribution is Gaussian by plotting several dimensions\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Plot histograms of the first 6 dimensions\n",
    "        for i in range(min(6, CONFIG[\"z_dim\"])):\n",
    "            axes[i].hist(z_np[:, i], bins=50, alpha=0.7, density=True)\n",
    "            axes[i].set_title(f\"Distribution of z[{i}]\")\n",
    "            \n",
    "            # Plot standard normal for comparison\n",
    "            x = np.linspace(-3, 3, 100)\n",
    "            axes[i].plot(x, np.exp(-x**2/2) / np.sqrt(2*np.pi), 'r-', lw=2)\n",
    "            axes[i].legend(['Standard Normal', 'Generated'])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CONFIG[\"base_dir\"], \"latent_distribution.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "        # Compute statistics of the latent distribution\n",
    "        z_mean = z_np.mean(axis=0)\n",
    "        z_var = z_np.var(axis=0)\n",
    "        \n",
    "        print(f\"Mean of z: {np.mean(z_mean):.4f} (should be close to 0)\")\n",
    "        print(f\"Variance of z: {np.mean(z_var):.4f} (should be close to 1)\")\n",
    "        \n",
    "        # Check covariance structure\n",
    "        z_cov = np.cov(z_np.T)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(z_cov, cmap='coolwarm')\n",
    "        plt.colorbar()\n",
    "        plt.title(\"Covariance Matrix of Latent Variables\")\n",
    "        plt.savefig(os.path.join(CONFIG[\"base_dir\"], \"latent_covariance.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "        # Check if values match well data\n",
    "        print(\"Checking conditioning on well data...\")\n",
    "        check_well_conditioning(z, num_samples=5)\n",
    "\n",
    "def check_well_conditioning(z: torch.Tensor, num_samples: int = 5):\n",
    "    \"\"\"Check if generated samples match the well data observations.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Generate images from latent codes\n",
    "        x = generator(z[:num_samples]).squeeze()  # [batch, channels, H, W]\n",
    "        \n",
    "        # Extract simulated observations at the well locations\n",
    "        gen = x.permute(0, 2, 3, 1)  # [batch, H, W, channels]\n",
    "        \n",
    "        # Collect simulated measurements at well locations\n",
    "        for i in range(num_samples):\n",
    "            print(f\"\\nSample {i+1} Well Values:\")\n",
    "            for j, (row, col) in enumerate(CONFIG[\"well_locations\"]):\n",
    "                sim_val = gen[i, row, col, :].cpu().numpy()\n",
    "                # Apply hard threshold for comparison\n",
    "                sim_val_binary = (sim_val > CONFIG[\"threshold\"]).astype(np.float32)\n",
    "                true_val = d_obs[j].cpu().numpy()\n",
    "                print(f\"  Well {j+1}:\")\n",
    "                print(f\"    Raw simulated = {sim_val}\")\n",
    "                print(f\"    Thresholded   = {sim_val_binary}\")\n",
    "                print(f\"    Observed      = {true_val}\")\n",
    "                \n",
    "                # Calculate match percentage\n",
    "                match_pct = np.mean(sim_val_binary == true_val) * 100\n",
    "                print(f\"    Match: {match_pct:.1f}%\")\n",
    "\n",
    "def plot_training_curves(metrics):\n",
    "    \"\"\"Plot training metrics over time.\"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(10, 16), sharex=True)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(metrics['epoch'], metrics['loss'])\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot likelihood and prior\n",
    "    axes[1].plot(metrics['epoch'], metrics['well_likelihood'], label='Well Likelihood')\n",
    "    axes[1].plot(metrics['epoch'], metrics['prior'], label='Prior')\n",
    "    axes[1].set_ylabel('Value')\n",
    "    axes[1].set_title('Well Likelihood and Prior')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Plot entropy\n",
    "    axes[2].plot(metrics['epoch'], metrics['entropy'])\n",
    "    axes[2].set_ylabel('Entropy')\n",
    "    axes[2].set_title('Estimated Entropy')\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    # Plot Gaussian metrics\n",
    "    mean_errors = [m[0] for m in metrics['gaussian_metrics']]\n",
    "    var_errors = [m[1] for m in metrics['gaussian_metrics']]\n",
    "    axes[3].plot(metrics['epoch'], mean_errors, label='Mean Error')\n",
    "    axes[3].plot(metrics['epoch'], var_errors, label='Variance Error')\n",
    "    axes[3].set_ylabel('Error')\n",
    "    axes[3].set_xlabel('Epoch')\n",
    "    axes[3].set_title('Gaussian Distribution Metrics')\n",
    "    axes[3].legend()\n",
    "    axes[3].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"base_dir\"], \"training_curves.png\"))\n",
    "    plt.show()\n",
    "\n",
    "###################################\n",
    "# 7. Main Execution\n",
    "###################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Run training\n",
    "    metrics = train()\n",
    "    \n",
    "    # Plot training progress\n",
    "    plot_training_curves(metrics)\n",
    "    \n",
    "    # Generate and visualize some samples\n",
    "    generate_and_visualize(num_samples=5)\n",
    "    \n",
    "    # Visualize latent distribution\n",
    "    visualize_latent_distribution()\n",
    "    \n",
    "    print(\"Training and visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092499a",
   "metadata": {},
   "source": [
    "#### History Matching Process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a7ced",
   "metadata": {},
   "source": [
    "This section implements the **Ensemble Smoother with Multiple Data Assimilation (ESMDA)** algorithm to condition geological models generated by a GAN to observed reservoir data.\n",
    "\n",
    "*   **Goal:** Iteratively update an ensemble of **latent vectors (`z`)** so that the geological models produced from these vectors by a pre-trained Generator match observed data from reservoir simulations.\n",
    "*   **Key Components:**\n",
    "    *   A **frozen, pre-trained Generator** that maps input vectors (`w`) to 3D geological models.\n",
    "    *   A **frozen, pre-trained Inference Network** which, in this script, is used to map the latent vectors (`z`) to the generator input vectors (`w`).\n",
    "    *   Observed **BHP timeseries data** from wells (`d_obs`).\n",
    "*   **ESMDA Process (Main Loop):**\n",
    "    1.  Start with an initial ensemble of latent vectors (`z_ensemble`).\n",
    "    2.  For a set number of iterations:\n",
    "        *   Map the current `z_ensemble` to a `w_ensemble` using the **Inference Network**.\n",
    "        *   Generate a batch of 3D geological models from the `w_ensemble` using the **Generator**.\n",
    "        *   Run **parallel reservoir simulations** for each generated model to obtain simulated well data (`d_model`).\n",
    "        *   Compare the simulated data to the observed data (`d_obs`).\n",
    "        *   Compute the **Kalman Gain** based on the ensemble covariance of the latent vectors (`z`) and the simulated data (`d_model`).\n",
    "        *   **Update** each latent vector in the `z_ensemble` using a Kalman-like update equation that combines the original latent vector, the Kalman Gain, the difference between the observed and simulated data, and added noise.\n",
    "        *   Save the updated `z_ensemble` and the corresponding `w_ensemble`.\n",
    "    3.  Track and save performance metrics like Mean Squared Error (MSE) between simulated and observed data.\n",
    "*   **Outcome:** The process yields a final ensemble of latent vectors (`z_ensemble`) and their corresponding `w_ensemble` vectors. These vectors, when fed into the Generator, produce geological realizations that are conditioned to the observed well data while maintaining diversity within the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List to store MSE results per iteration for later analysis.\n",
    "mse_history = []\n",
    "\n",
    "# Configuration dictionary holding all parameters for the script.\n",
    "CONFIG = {\n",
    "    # --- Paths ---\n",
    "    \"base_dir\": \"D:/Generalizedized_GAN/ESMDA/\", # Base directory for checkpoints and data\n",
    "    \"checkpoint_path\": \"checkpoint_epoch_3000.pt\", # Path to the Generator checkpoint\n",
    "\n",
    "    # --- Network parameters ---\n",
    "    # Dimensions for the GAN (Generator) and Inference Network.\n",
    "    \"w_dim\": 100,          # Dimension of the noise vector fed into the Generator\n",
    "    \"z_dim\": 100,          # Dimension of the latent space used by the Inference Network\n",
    "    \"hidden_dim\": 256,     # Dimension of hidden layers in networks (if applicable)\n",
    "    \"Final_iter_IN\": 400,    # Number of epochs for training (used to find inference net checkpoint name)\n",
    "}\n",
    "\n",
    "# --- Device Setup ---\n",
    "# Determine whether to use GPU (CUDA) if available, otherwise use CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Model Loading ---\n",
    "# Load the pre-trained Generator model.\n",
    "# This model takes 'w' and generates subsurface models.\n",
    "generator_checkpoint_path = os.path.join(CONFIG[\"base_dir\"], CONFIG[\"checkpoint_path\"])\n",
    "print(f\"Loading generator from {generator_checkpoint_path}\")\n",
    "generator = load_generator(generator_checkpoint_path, device)\n",
    "generator.eval() # Set the generator to evaluation mode\n",
    "\n",
    "# Load the pre-trained Inference Network model.\n",
    "# This model takes 'z' and outputs 'w'.\n",
    "inference_net_checkpoint_path = os.path.join(CONFIG[\"base_dir\"], f\"inference_net_epoch_{CONFIG['Final_iter_IN']}.pt\")\n",
    "print(f\"Loading inference network from {inference_net_checkpoint_path}\")\n",
    "inference_net = InferenceNet(\n",
    "    CONFIG[\"w_dim\"], \n",
    "    CONFIG[\"z_dim\"], \n",
    "    CONFIG[\"hidden_dim\"]\n",
    ").to(device)\n",
    "\n",
    "# Load model weights into the inference network.\n",
    "try:\n",
    "    checkpoint = torch.load(inference_net_checkpoint_path, map_location=device)\n",
    "    inference_net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    inference_net.eval()  # Set the inference network to evaluation mode\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Inference network checkpoint not found at {inference_net_checkpoint_path}\")\n",
    "    # Handle the error appropriately, e.g., exit or train the network\n",
    "    exit()\n",
    "except KeyError:\n",
    "     print(f\"Error: 'model_state_dict' not found in checkpoint file {inference_net_checkpoint_path}. Check the checkpoint structure.\")\n",
    "     exit()\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "# This ensures the code inside only runs when the script is executed directly\n",
    "# (not when imported as a module).\n",
    "if __name__ == '__main__':\n",
    "    print(\"Main: Starting ESMDA process.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. LOAD AND STANDARDIZE OBSERVATION DATA\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"Loading and standardizing observation data...\")\n",
    "    # Load the observed BHP data from CSV files.\n",
    "    # We assume these are the 'truth' observations to assimilate against.\n",
    "    \n",
    "    # Load Injector 1 BHP data\n",
    "    InJ_data = pd.read_csv(\"Truth/Inj 1_CMG_aligned.csv\")['BHP'].values\n",
    "    # Standardize the data: Subtract mean and divide by standard deviation.\n",
    "    # Store the mean and std of the *truth* data, as simulated data will be\n",
    "    # standardized using these same values to make them comparable.\n",
    "    InJ_mean, InJ_std = np.mean(InJ_data), np.std(InJ_data)\n",
    "    InJ_obs = (InJ_data - InJ_mean) / InJ_std\n",
    "\n",
    "    # Load Producer 1 BHP data\n",
    "    P1_data = pd.read_csv(\"Truth/Prod 1_CMG_aligned.csv\")['BHP'].values\n",
    "    P1_mean, P1_std = np.mean(P1_data), np.std(P1_data)\n",
    "    P1_obs = (P1_data - P1_mean) / P1_std\n",
    "    \n",
    "    # Load Producer 2 BHP data\n",
    "    P2_data = pd.read_csv(\"Truth/Prod 2_CMG_aligned.csv\")['BHP'].values\n",
    "    P2_mean, P2_std = np.mean(P2_data), np.std(P2_data)\n",
    "    P2_obs = (P2_data - P2_mean) / P2_std\n",
    "\n",
    "    # Concatenate all standardized observation data into a single vector.\n",
    "    d_obs = np.concatenate([InJ_data, P1_data, P2_data]) # Store original for MSE\n",
    "    d_obs_std = np.concatenate([InJ_obs, P1_obs, P2_obs]) # Standardized for assimilation\n",
    "    n_obs = d_obs_std.shape[0] # Total number of observations\n",
    "\n",
    "    print(f\"Loaded {n_obs} observation points.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. ESMDA PARAMETERS & INITIAL ENSEMBLE SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"Setting up ESMDA parameters...\")\n",
    "    n_iter = 25 # Number of assimilation (ESMDA) iterations to perform\n",
    "    \n",
    "    # Define the sequence of alpha values for each iteration.\n",
    "    # In standard ESMDA, alpha_k = N_iter for all k=1,...,N_iter.\n",
    "    alpha_list = [n_iter] * n_iter \n",
    "\n",
    "    # Define the original observation error standard deviation (sigma).\n",
    "    # This represents the uncertainty in the measured data.\n",
    "    sigma = 0.06 \n",
    "    # The covariance matrix of observation error (Gamma).\n",
    "    # Assuming errors are uncorrelated and have the same variance.\n",
    "    # This needs to be in the *standardized* data space for the update step.\n",
    "    # Since the data is standardized by dividing by its std dev (which is ~1\n",
    "    # for the whole concatenated vector), the scaled sigma_std is approximately sigma.\n",
    "    # A more rigorous approach would scale sigma by the effective std dev of d_obs.\n",
    "    # For simplicity here, we assume sigma_std = sigma.\n",
    "    sigma_std = sigma  # Keep sigma as is, assuming it's relative to the standardized scale\n",
    "    Gamma_std = np.diag([sigma_std**2] * n_obs) # Diagonal covariance matrix\n",
    "\n",
    "    # Define the ensemble size.\n",
    "    ensemble_size = 200\n",
    "    latent_dim = CONFIG[\"z_dim\"] # Dimension of the latent space (z)\n",
    "\n",
    "    # Initialize the latent variable ensemble (z_ensemble).\n",
    "    # Each row is a member of the ensemble.\n",
    "    # Initialized from a standard normal distribution.\n",
    "    # The commented-out line shows how to load a previously saved ensemble.\n",
    "    z_ensemble = torch.randn(ensemble_size, latent_dim, device=device) \n",
    "    # z_ensemble = torch.load(\"z_ensemble_iter14.pt\", map_location=device)\n",
    "    \n",
    "    print(f\"Ensemble size: {ensemble_size}, Latent dimension: {latent_dim}\")\n",
    "    print(f\"Number of ESMDA iterations: {n_iter}, Initial sigma: {sigma}\")\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. MAIN LOOP OVER ESMDA ITERATIONS\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"Starting ESMDA iterations...\")\n",
    "    \n",
    "    # Adjust starting iteration if resuming (e.g., from loaded z_ensemble)\n",
    "    start_iter = 0 # Change this if resuming from a specific iteration\n",
    "    \n",
    "    for k in range(start_iter, n_iter):\n",
    "        current_iteration = k + 1\n",
    "        print(f\"\\n--- ESMDA Iteration {current_iteration}/{n_iter} ---\")\n",
    "\n",
    "        # --- Map latent variable 'z' to generator input 'w' using the Inference Network ---\n",
    "        # This is specific to this GAN setup where G(w) generates samples and\n",
    "        # the inference network learns a mapping z -> w.\n",
    "        w_ensemble = inference_net(z_ensemble)\n",
    "        \n",
    "        # --- Generate subsurface models from the 'w' ensemble using the Generator ---\n",
    "        # generator(w_ensemble) produces outputs likely of shape (ensemble_size, C, H, W)\n",
    "        # .detach().cpu().numpy() converts the torch tensor to a numpy array on CPU.\n",
    "        generated_batch = generator(w_ensemble).detach().cpu().numpy()\n",
    "        \n",
    "        # Apply a thresholding function if needed (e.g., for binary or categorical models).\n",
    "        # Assumes threshold_samples is defined in Utilities.\n",
    "        generated_batch = threshold_samples(generated_batch.squeeze())\n",
    "        \n",
    "        # Re-order dimensions for the simulation code if needed.\n",
    "        # From (ensemble_size, Nx, Ny) or (ensemble_size, Channels, Nx, Ny)\n",
    "        # to (ensemble_size, Nx, Ny, Channels).\n",
    "        # Assuming the generator output is (N, 1, H, W) after squeeze -> (N, H, W)\n",
    "        # and simulation expects (N, H, W, 1).\n",
    "        Z = generated_batch[:, :, :, np.newaxis] # Add channel dimension back\n",
    "\n",
    "        # Set the number of processors for parallel simulations.\n",
    "        # Limit to ensemble size and a reasonable maximum (e.g., 128).\n",
    "        num_processors = min(multiprocessing.cpu_count(), ensemble_size, 128)\n",
    "        print(f\"Using {num_processors} processors for forward simulations.\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3A. RUN FORWARD SIMULATIONS IN PARALLEL\n",
    "        # ---------------------------------------------------------------------\n",
    "        print(\"Running forward simulations...\")\n",
    "        processes = []\n",
    "        completed_indices = []\n",
    "        batch_size = num_processors # Process in batches equal to the number of available processors\n",
    "        \n",
    "        for batch_start in range(0, ensemble_size, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, ensemble_size)\n",
    "            current_batch_indices = range(batch_start, batch_end)\n",
    "            print(f\"Launching simulation batch for indices {batch_start} to {batch_end-1}...\")\n",
    "            \n",
    "            # Launch processes for the current batch\n",
    "            for i in current_batch_indices:\n",
    "                # The 'worker' function (assumed from Utilities) takes the ensemble\n",
    "                # index and the specific subsurface model data for that member.\n",
    "                # It's responsible for setting up and running the simulator, saving\n",
    "                # the results (e.g., to CSV files).\n",
    "                p = multiprocessing.Process(target=worker, args=(i, Z[i]))\n",
    "                p.start()\n",
    "                processes.append((i, p))\n",
    "            \n",
    "            # Wait for all processes in this batch to complete\n",
    "            for idx, p in processes:\n",
    "                p.join() # Wait for process to finish\n",
    "                completed_indices.append(idx)\n",
    "            processes = [] # Clear the list for the next batch\n",
    "            \n",
    "            print(f\"Completed simulation batch starting at index {batch_start}.\")\n",
    "        \n",
    "        # Verify that simulations ran for all ensemble members.\n",
    "        if len(completed_indices) != ensemble_size:\n",
    "            raise RuntimeError(\n",
    "                f\"Expected {ensemble_size} completed processes, but only {len(completed_indices)} finished. Check simulation logs.\"\n",
    "            )\n",
    "        print(\"All forward simulations completed.\")\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3B. COLLECT SIMULATION RESULTS & STANDARDIZE\n",
    "        # ---------------------------------------------------------------------\n",
    "        print(\"Collecting and standardizing simulation results...\")\n",
    "        d_model = []\n",
    "        # Define a backup path for the simulation results\n",
    "        backup_path = \"simulation_results_backup/\" \n",
    "        os.makedirs(backup_path, exist_ok=True) # Create backup directory if it doesn't exist\n",
    "\n",
    "        for i in range(ensemble_size):\n",
    "            try:\n",
    "                # Define paths to the simulation output files for ensemble member i\n",
    "                # These paths should match where the 'worker' function saves the results.\n",
    "                sim_InJ_file = f\"wells_timeseries_alined/Inj 1_Ensemble{i}_Aligned.csv\"\n",
    "                sim_P1_file = f\"wells_timeseries_alined/Prod 1_Ensemble{i}_Aligned.csv\"\n",
    "                sim_P2_file = f\"wells_timeseries_alined/Prod 2_Ensemble{i}_Aligned.csv\"\n",
    "\n",
    "                # Load the simulated BHP data\n",
    "                sim_InJ_data = pd.read_csv(sim_InJ_file)['BHP'].values\n",
    "                sim_P1_data = pd.read_csv(sim_P1_file)['BHP'].values\n",
    "                sim_P2_data = pd.read_csv(sim_P2_file)['BHP'].values\n",
    "                \n",
    "                # Standardize the *simulated* data using the mean and std dev of the *truth* data.\n",
    "                # This makes the simulated data statistically comparable to the observed data.\n",
    "                sim_InJ_std = (sim_InJ_data - InJ_mean) / InJ_std\n",
    "                sim_P1_std = (sim_P1_data - P1_mean) / P1_std\n",
    "                sim_P2_std = (sim_P2_data - P2_mean) / P2_std\n",
    "\n",
    "                # Concatenate the standardized simulated data for this ensemble member.\n",
    "                sim_concat_std = np.concatenate([sim_InJ_std, sim_P1_std, sim_P2_std])\n",
    "                d_model.append(sim_concat_std) # Append the standardized result\n",
    "                \n",
    "                # Backup the original simulation output files\n",
    "                # Appending iteration number to filename helps track history\n",
    "                shutil.copy(sim_InJ_file, os.path.join(backup_path, f\"Inj 1_Ensemble{i}_Aligned_iter{current_iteration}.csv\"))\n",
    "                shutil.copy(sim_P1_file, os.path.join(backup_path, f\"Prod 1_Ensemble{i}_Aligned_iter{current_iteration}.csv\"))\n",
    "                shutil.copy(sim_P2_file, os.path.join(backup_path, f\"Prod 2_Ensemble{i}_Aligned_iter{current_iteration}.csv\"))\n",
    "\n",
    "                # Clean up the original simulation output files to save space\n",
    "                os.remove(sim_InJ_file)\n",
    "                os.remove(sim_P1_file)\n",
    "                os.remove(sim_P2_file)\n",
    "\n",
    "            except FileNotFoundError as e:\n",
    "                 print(f\"Error: Simulation output file not found for ensemble {i}: {e}\")\n",
    "                 # This indicates a simulation failed to produce output.\n",
    "                 # The script cannot proceed; raise the error.\n",
    "                 raise\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading or processing simulation output for ensemble {i}: {e}\")\n",
    "                raise\n",
    "        \n",
    "        # Convert the list of simulation results to a numpy array.\n",
    "        d_model_std = np.array(d_model)  # shape: (ensemble_size, n_obs)\n",
    "        print(\"Simulation results collected and standardized.\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3C. COMPUTE MSE IN ORIGINAL SPACE (FOR DIAGNOSTICS)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Note: This calculates MSE on the *unstandardized* data (d_obs vs\n",
    "        # *unstandardized* d_model) for interpretability, even though the\n",
    "        # assimilation happens on the standardized data. We need to reload\n",
    "        # the unstandardized data for this calculation.\n",
    "        d_model_orig = []\n",
    "        for i in range(ensemble_size):\n",
    "             # Reload original (unstandardized) data from backup\n",
    "            sim_InJ_file = os.path.join(backup_path, f\"Inj 1_Ensemble{i}_Aligned_iter{current_iteration}.csv\")\n",
    "            sim_P1_file = os.path.join(backup_path, f\"Prod 1_Ensemble{i}_Aligned_iter{current_iteration}.csv\")\n",
    "            sim_P2_file = os.path.join(backup_path, f\"Prod 2_Ensemble{i}_Aligned_iter{current_iteration}.csv\")\n",
    "            \n",
    "            sim_InJ_data = pd.read_csv(sim_InJ_file)['BHP'].values\n",
    "            sim_P1_data = pd.read_csv(sim_P1_file)['BHP'].values\n",
    "            sim_P2_data = pd.read_csv(sim_P2_file)['BHP'].values\n",
    "            d_model_orig.append(np.concatenate([sim_InJ_data, sim_P1_data, sim_P2_data]))\n",
    "            \n",
    "        d_model_orig = np.array(d_model_orig)\n",
    "\n",
    "        # Calculate Mean Squared Error between truth observations and each simulation.\n",
    "        mse_errors = [mean_squared_error(d_obs, d_model_orig[i]) for i in range(ensemble_size)]\n",
    "        mean_mse = np.mean(mse_errors)\n",
    "        print(f\"Mean MSE (original space) for iteration {current_iteration}: {mean_mse:.6f}\")\n",
    "        # Store the MSE values for this iteration.\n",
    "        mse_history.append({'iteration': current_iteration, 'mean_mse': mean_mse, 'all_mses': mse_errors})\n",
    "\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3D. COMPUTE STATISTICS & FORM KALMAN GAIN IN STANDARDIZED SPACE\n",
    "        # ---------------------------------------------------------------------\n",
    "        print(\"Computing ensemble statistics and Kalman Gain...\")\n",
    "        # Convert latent ensemble tensor to numpy for calculations.\n",
    "        z_np = z_ensemble.cpu().numpy()\n",
    "        \n",
    "        # Compute ensemble mean of the latent variables (z).\n",
    "        z_mean = np.mean(z_np, axis=0)\n",
    "        # Compute anomalies (deviations from the mean) for z.\n",
    "        A_z = z_np - z_mean\n",
    "        \n",
    "        # Compute ensemble mean of the standardized simulation results (d_model_std).\n",
    "        d_mean_std = np.mean(d_model_std, axis=0)\n",
    "        # Compute anomalies (deviations from the mean) for d_model_std.\n",
    "        A_d_std = d_model_std - d_mean_std\n",
    "        \n",
    "        # Compute sample covariance between z anomalies and d anomalies.\n",
    "        # Pzd = (A_z.T @ A_d_std) / (N-1)\n",
    "        Cov_zd_std = (A_z.T @ A_d_std) / (ensemble_size - 1)\n",
    "        \n",
    "        # Compute sample covariance of d anomalies.\n",
    "        # Pdd = (A_d_std.T @ A_d_std) / (N-1)\n",
    "        Cov_dd_std = (A_d_std.T @ A_d_std) / (ensemble_size - 1)\n",
    "        \n",
    "        # Get the current alpha value for this ESMDA iteration.\n",
    "        alpha = alpha_list[k]\n",
    "        \n",
    "        # Compute the Kalman Gain (K) in the standardized data space.\n",
    "        # K = Pzd @ (Pdd + alpha * Gamma_std)^-1\n",
    "        # Using np.linalg.pinv for robustness against singular matrices.\n",
    "        K_std = Cov_zd_std @ np.linalg.pinv(Cov_dd_std + alpha * Gamma_std)\n",
    "        print(\"Kalman Gain computed.\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3E. UPDATE EACH ENSEMBLE MEMBER IN STANDARDIZED SPACE\n",
    "        # ---------------------------------------------------------------------\n",
    "        print(\"Updating latent ensemble members...\")\n",
    "        # Create a copy of the ensemble before update to measure change.\n",
    "        z_np_before_update = np.copy(z_np) \n",
    "\n",
    "        # Apply the ESMDA update formula to each ensemble member.\n",
    "        # zi_new = zi_old + K * (d_obs_std + sqrt(alpha) * epsilon_i - di_old_std)\n",
    "        # Note: The epsilon term is already scaled by sqrt(alpha) implicitly\n",
    "        # when sampling from a distribution with covariance alpha * Gamma_std.\n",
    "        \n",
    "        for i in range(ensemble_size):\n",
    "            # Generate an observation error realization for this ensemble member.\n",
    "            # The error is sampled from a multivariate normal distribution\n",
    "            # with mean 0 and covariance alpha * Gamma_std.\n",
    "            eps_std = np.random.multivariate_normal(\n",
    "                mean=np.zeros(n_obs), cov=alpha * Gamma_std\n",
    "            )\n",
    "            # Compute the innovation term (difference between observed data + noise and simulated data).\n",
    "            innovation_std = d_obs_std + eps_std - d_model_std[i]\n",
    "            # Apply the update formula.\n",
    "            z_np[i] = z_np[i] + (K_std @ innovation_std)\n",
    "            \n",
    "        # Calculate the average magnitude of the change in z for diagnostics.\n",
    "        z_change = np.mean(np.abs(z_np - z_np_before_update))\n",
    "        print(f\"Average |z| for iteration {current_iteration}: {z_change:.6f}\")\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3F. SAVE UPDATED LATENT ENSEMBLE AND W ENSEMBLE\n",
    "        # ---------------------------------------------------------------------\n",
    "        print(\"Saving updated ensembles...\")\n",
    "        # Convert the updated numpy array back to a torch tensor for the next iteration.\n",
    "        z_ensemble = torch.tensor(z_np, device=device, dtype=torch.float32)\n",
    "        \n",
    "        # Define filenames for saving.\n",
    "        # Appending iteration number to track history.\n",
    "        # Saving w_ensemble is useful for debugging or if you wanted to regenerate\n",
    "        # models directly from w without re-running the inference network.\n",
    "        w_ensemble_filename = f\"w_ensemble_iter{current_iteration}.pt\"\n",
    "        z_ensemble_filename = f\"z_ensemble_iter{current_iteration}.pt\"\n",
    "        \n",
    "        # Save the ensembles to files.\n",
    "        # .cpu() moves the tensor to CPU memory before saving, which is generally recommended.\n",
    "        torch.save(w_ensemble.cpu(), w_ensemble_filename)\n",
    "        torch.save(z_ensemble.cpu(), z_ensemble_filename)\n",
    "        \n",
    "        print(f\"Ensembles saved: {w_ensemble_filename}, {z_ensemble_filename}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. SAVE MSE HISTORY TO CSV\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\nESMDA process finished.\")\n",
    "    print(\"Saving MSE history...\")\n",
    "    # Convert the list of MSE dictionaries into a pandas DataFrame.\n",
    "    # Note: 'all_mses' will store lists, which might make the CSV less readable\n",
    "    # directly but preserves all individual MSE values. You might choose to\n",
    "    # save only 'mean_mse' if desired.\n",
    "    mse_df = pd.DataFrame(mse_history)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file.\n",
    "    mse_csv_path = \"mse_history.csv\"\n",
    "    mse_df.to_csv(mse_csv_path, index=False)\n",
    "    print(f\"MSE history saved to {mse_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
